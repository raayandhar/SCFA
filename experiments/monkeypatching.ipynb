{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's examine the effect of replacing Llama's dense attention with the hash attention kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/important/Triton/SFCA/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"cuda:2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm doing well, thank you for asking. I'm a large language model, so I don\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "]\n",
    "model_input = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\", add_generation_prompt = True).to(model.device)\n",
    "response_tensor = model.generate(model_input, max_new_tokens = 20, temperature = 0.7)\n",
    "tokenizer.decode(response_tensor[0][len(model_input[0]):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing attention modules: 100%|██████████| 16/16 [00:00<00:00, 20.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention, LlamaConfig, Cache, FlashAttentionKwargs, apply_rotary_pos_emb, repeat_kv\n",
    "from typing import Tuple, Optional, Unpack\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "def vector_hash_fn(x: torch.Tensor, num_buckets: int, R: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: (..., D)\n",
    "    R: (D, b/2)\n",
    "    \"\"\"\n",
    "    D = x.shape[-1]\n",
    "    assert R.shape == (D, num_buckets // 2)\n",
    "    return torch.argmax(torch.cat([x @ R, -x @ R], dim=-1), dim=-1)\n",
    "\n",
    "def get_vector_hash(D: int, num_buckets: int, device: torch.device = \"cpu\", dtype: torch.dtype = torch.bfloat16) -> torch.Tensor:\n",
    "    R = torch.randn(D, num_buckets // 2, device = device, dtype = dtype)\n",
    "    return lambda x: vector_hash_fn(x, num_buckets, R)\n",
    "\n",
    "def reference_hash_attn(q, k, v, num_buckets: int, sm_scale: float = 1.0, vector_hash = None):\n",
    "    \"\"\"\n",
    "    q: (B, H, N, D)\n",
    "    k: (B, H, N, D)\n",
    "    v: (B, H, N, D)\n",
    "\n",
    "    Note: This implementation sucks! It's just a sanity check.\n",
    "    \"\"\"\n",
    "    assert num_buckets % 2 == 0, \"num_buckets must be even\"\n",
    "    if vector_hash is None:\n",
    "        vector_hash = get_vector_hash(D = q.shape[-1], num_buckets = num_buckets, device = q.device)\n",
    "    B, H, N, D = q.shape\n",
    "    q_hashes = vector_hash(q) # (B, H, N)\n",
    "    k_hashes = vector_hash(k) # (B, H, N)\n",
    "    out = torch.zeros_like(q)\n",
    "    for i in range(num_buckets):\n",
    "        for b in range(B):\n",
    "            for h in range(H):\n",
    "                q_mask = (q_hashes[b][h] == i) # (N)\n",
    "                k_mask = (k_hashes[b][h] == i) # (N)\n",
    "                q_indices = torch.nonzero(q_mask, as_tuple=False).squeeze() # (N)\n",
    "                k_indices = torch.nonzero(k_mask, as_tuple=False).squeeze() # (N)\n",
    "                if len(q_indices.shape) == 0 or len(k_indices.shape) == 0:\n",
    "                    continue\n",
    "                q_bucket = q[b, h, q_indices] # (N, D)\n",
    "                k_bucket = k[b, h, k_indices] # (N, D)\n",
    "                v_bucket = v[b, h, k_indices] # (N, D)\n",
    "                attn_mask = q_indices.unsqueeze(-1) >= k_indices.unsqueeze(-2)\n",
    "                attn_scores = torch.matmul(q_bucket, k_bucket.transpose(-2, -1)) * sm_scale\n",
    "                attn_scores = attn_scores.masked_fill(~attn_mask, float(\"-inf\"))\n",
    "                attn = F.softmax(attn_scores, dim=-1)\n",
    "                attn = attn.nan_to_num(0.0) # some cols will be totally masked out and softmax will produce NaNs\n",
    "                # sns.heatmap(attn.cpu().numpy().squeeze(), annot = False, mask = ~attn_mask.cpu().numpy().squeeze())\n",
    "                # plt.show()\n",
    "                # return\n",
    "                partial_prod = torch.matmul(attn, v_bucket)\n",
    "                out[b, h, q_indices] += partial_prod.squeeze(0)\n",
    "    return out\n",
    "\n",
    "class HashAttention(LlamaAttention):\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int, num_buckets: int, device):\n",
    "        super().__init__(config, layer_idx)\n",
    "        self.vector_hash = get_vector_hash(D = self.head_dim, num_buckets = num_buckets, device = device)\n",
    "        self.num_buckets = num_buckets\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor, # (batch_size, seq_len, hidden_size)\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        \"\"\"\n",
    "        Shapes:\n",
    "        query_states: (batch_size, num_heads, seq_len, head_dim)\n",
    "        key_states: (batch_size, num_heads, seq_len, head_dim)\n",
    "        value_states: (batch_size, num_heads, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        attn_output = reference_hash_attn(\n",
    "            q = query_states,\n",
    "            k = repeat_kv(key_states, self.num_key_value_groups),\n",
    "            v = repeat_kv(value_states, self.num_key_value_groups),\n",
    "            num_buckets = self.num_buckets,\n",
    "            sm_scale = self.scaling,\n",
    "            vector_hash = self.vector_hash,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output, None\n",
    "    \n",
    "# Monkeypatch time\n",
    "hash_model = deepcopy(model).to(model.device)\n",
    "NUM_BUCKETS = 16\n",
    "n_modules_to_replace = len(list(filter(lambda x: isinstance(x, LlamaAttention) or isinstance(x, HashAttention), model.modules())))\n",
    "with tqdm(total = n_modules_to_replace, desc = \"Replacing attention modules\") as pbar:\n",
    "    for name, module in hash_model.named_modules():\n",
    "        if isinstance(module, LlamaAttention) or isinstance(module, HashAttention):\n",
    "            # Construct new module\n",
    "            new_attn_module = HashAttention(config = module.config, layer_idx = module.layer_idx, num_buckets = NUM_BUCKETS, device = model.device)\n",
    "            new_attn_module.load_state_dict(module.state_dict())\n",
    "            new_attn_module.to(hash_model.device).to(torch.bfloat16)\n",
    "\n",
    "            # Split full name to find parent module\n",
    "            parent_module = hash_model\n",
    "            parent_name_parts = name.split('.')\n",
    "            child_name = parent_name_parts[-1]\n",
    "\n",
    "            if len(parent_name_parts) > 1:\n",
    "                for part in parent_name_parts[:-1]:\n",
    "                    if part.isdigit(): # Handles modules in nn.Sequential or nn.ModuleList\n",
    "                        parent_module = parent_module[int(part)]\n",
    "                    else:\n",
    "                        parent_module = getattr(parent_module, part)\n",
    "\n",
    "            setattr(parent_module, child_name, new_attn_module)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): HashAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The final. (pause.\\n\\nThis is the, for the,    isotope the,'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "]\n",
    "model_input = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\", add_generation_prompt = True).to(model.device)\n",
    "response_tensor = hash_model.generate(model_input, max_new_tokens = 20)\n",
    "tokenizer.decode(response_tensor[0][len(model_input[0]):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
