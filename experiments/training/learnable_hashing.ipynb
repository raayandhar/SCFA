{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we make the random projection hashable?\n",
    "\n",
    "Inspired by [HashAttention: Semantic Sparsity for Faster Inference](https://arxiv.org/pdf/2412.14468) by Aditya Desai et al.\n",
    "### Random hashing:\n",
    "\n",
    "We hash $Q$ and $K$ into $B$ buckets by using a random matrix $R \\in \\mathbb{R}^{d \\times b/2}$ and taking $\\argmax([xR; −xR])$. As per the Johnson-Lindenstrauss lemma, this maps $Q$ and $K$ into a $b/2$-dimensional space in a relative-distance preserving way.\n",
    "\n",
    "Effectively, our vector $\\vec{x} \\in \\mathbb{R}^d$ is projected to hash vector $\\vec{x}_h \\in \\mathbb{R}^{b/2}$, and we take its dimension with the largest component as the hash index.\n",
    "\n",
    "### Learnable hashing:\n",
    "\n",
    "The model can learn its own hash functions that cluster together attendant queries/keys based on semantic similarity rather than vector distance (e.g this does processing work). We want to encourage two things:\n",
    "1. Downstream performance\n",
    "2. Balancing loss (to prevent degenerate solutions where all q/ks are mapped to the same hash index):\n",
    "    - Entropy regularization?\n",
    "    - Regularization on projection matrix (L1) to prevent one dim from dominating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnableHash(nn.Module):\n",
    "    def __init__(self, D: int, num_buckets: int, device: torch.device = \"cpu\", dtype: torch.dtype = torch.bfloat16):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.num_buckets = num_buckets\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.R = nn.Parameter(torch.randn(D, num_buckets // 2, device = device, dtype = dtype), requires_grad = True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, tau: float = 1.0) -> torch.Tensor:\n",
    "        proj = torch.matmul(x, self.R)\n",
    "        logits = torch.cat([proj, -proj], dim=-1)\n",
    "        one_hot = F.gumbel_softmax(logits, tau = tau, hard = True)\n",
    "        return torch.argmax(one_hot, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Learnable Hashing, no balancing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention, LlamaConfig, Cache, FlashAttentionKwargs, apply_rotary_pos_emb, repeat_kv\n",
    "from typing import Tuple, Optional, Unpack\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from source.scfa_paper.scfa_wrapper import hash_sparse_attention\n",
    "\n",
    "def scfa_hash_attn(q, k, v, num_buckets: int, sm_scale: float = 1.0, vector_hash: nn.Module = None):\n",
    "    \"\"\"\n",
    "    q: (B, H, N, D)\n",
    "    k: (B, H, N, D)\n",
    "    v: (B, H, N, D)\n",
    "    \"\"\"\n",
    "    B, H, N, D = q.shape\n",
    "    assert num_buckets % 2 == 0, \"num_buckets must be even\"\n",
    "    q_hashes = vector_hash.forward(q).to(torch.int32).transpose(1, 2).contiguous() # (B, N, H)\n",
    "    k_hashes = vector_hash.forward(k).to(torch.int32).transpose(1, 2).contiguous() # (B, N, H)\n",
    "    assert q_hashes.max() < num_buckets, \"q_hashes must be less than num_buckets\"\n",
    "    assert k_hashes.max() < num_buckets, \"k_hashes must be less than num_buckets\"\n",
    "    assert q_hashes.min() >= 0, \"q_hashes must be non-negative\"\n",
    "    assert k_hashes.min() >= 0, \"k_hashes must be non-negative\"\n",
    "    scfa_q = q.transpose(1, 2).contiguous() # (B, N, H, D)\n",
    "    scfa_k = k.transpose(1, 2).contiguous() # (B, N, H, D)\n",
    "    scfa_v = v.transpose(1, 2).contiguous() # (B, N, H, D)\n",
    "    return hash_sparse_attention(scfa_q, scfa_k, scfa_v, q_hashes, k_hashes, sm_scale).transpose(1, 2).contiguous()\n",
    "\n",
    "class HashAttention(LlamaAttention):\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int, num_buckets: int, device):\n",
    "        super().__init__(config, layer_idx)\n",
    "        self.vector_hash = LearnableHash(D = self.head_dim, num_buckets = num_buckets, device = device)\n",
    "        self.num_buckets = num_buckets\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor, # (batch_size, seq_len, hidden_size)\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        \"\"\"\n",
    "        Shapes:\n",
    "        query_states: (batch_size, num_heads, seq_len, head_dim)\n",
    "        key_states: (batch_size, num_heads, seq_len, head_dim)\n",
    "        value_states: (batch_size, num_heads, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            if past_key_value is not None:\n",
    "                # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "                cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "                key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "            attn_output = scfa_hash_attn(\n",
    "                q = query_states,\n",
    "                k = repeat_kv(key_states, self.num_key_value_groups),\n",
    "                v = repeat_kv(value_states, self.num_key_value_groups),\n",
    "                num_buckets = self.num_buckets,\n",
    "                sm_scale = self.scaling,\n",
    "                vector_hash = self.vector_hash,\n",
    "            )\n",
    "\n",
    "            attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "            attn_output = self.o_proj(attn_output)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(query_states.shape, self.num_buckets)\n",
    "            print(e)\n",
    "        \n",
    "        return attn_output, None\n",
    "    \n",
    "# Monkeypatch time\n",
    "def monkeypatch(model, num_buckets: int, one_at_a_time: bool = False):\n",
    "    n_modules_to_replace = len(list(filter(lambda x: isinstance(x, LlamaAttention), model.modules())))\n",
    "    pbar = tqdm(total = n_modules_to_replace, desc = \"Replacing attention modules\")\n",
    "    if one_at_a_time:\n",
    "        pbar.disable = True    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LlamaAttention):\n",
    "            # Construct new module\n",
    "            new_attn_module = HashAttention(config = module.config, layer_idx = module.layer_idx, num_buckets = num_buckets, device = model.device)\n",
    "            new_attn_module.load_state_dict(module.state_dict(), strict = False) # random hash matrices are newly initialized\n",
    "            new_attn_module.to(model.device).to(torch.bfloat16)\n",
    "\n",
    "            # Split full name to find parent module\n",
    "            parent_module = model\n",
    "            parent_name_parts = name.split('.')\n",
    "            child_name = parent_name_parts[-1]\n",
    "\n",
    "            if len(parent_name_parts) > 1:\n",
    "                for part in parent_name_parts[:-1]:\n",
    "                    if part.isdigit(): # Handles modules in nn.Sequential or nn.ModuleList\n",
    "                        parent_module = parent_module[int(part)]\n",
    "                    else:\n",
    "                        parent_module = getattr(parent_module, part)\n",
    "\n",
    "            setattr(parent_module, child_name, new_attn_module)\n",
    "            if one_at_a_time:\n",
    "                return\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd706173055f4d67a5ed797de95ab8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 4.3298, PPL: 94.00: 100%|██████████| 1000/1000.0 [00:54<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.9514, PPL: 70.14: 100%|██████████| 1000/1000.0 [00:53<00:00, 18.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.8711, PPL: 53.32: 100%|██████████| 1000/1000.0 [00:54<00:00, 18.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.6904, PPL: 49.49: 100%|██████████| 1000/1000.0 [00:54<00:00, 18.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.5899, PPL: 43.79: 100%|██████████| 1000/1000.0 [00:55<00:00, 17.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.5997, PPL: 40.12: 100%|██████████| 1000/1000.0 [00:55<00:00, 17.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.5356, PPL: 42.00: 100%|██████████| 1000/1000.0 [00:54<00:00, 18.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.5259, PPL: 39.82: 100%|██████████| 1000/1000.0 [00:53<00:00, 18.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.6034, PPL: 42.07: 100%|██████████| 1000/1000.0 [00:54<00:00, 18.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.5802, PPL: 44.42: 100%|██████████| 1000/1000.0 [00:55<00:00, 17.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.4661, PPL: 36.47: 100%|██████████| 1000/1000.0 [00:58<00:00, 17.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.5791, PPL: 42.09: 100%|██████████| 1000/1000.0 [00:56<00:00, 17.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.5560, PPL: 42.68: 100%|██████████| 1000/1000.0 [00:59<00:00, 16.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.4123, PPL: 35.73: 100%|██████████| 1000/1000.0 [00:59<00:00, 16.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.5928, PPL: 45.91: 100%|██████████| 1000/1000.0 [00:59<00:00, 16.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 LlamaAttention modules remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss: 3.4820, PPL: 38.59: 100%|██████████| 1000/1000.0 [01:00<00:00, 16.63it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "fineweb_dataset = datasets.load_dataset(\"HuggingFaceFW/fineweb\", \"sample-10BT\", split=\"train\", streaming=True)\n",
    "hash_model = deepcopy(model).to(model.device)\n",
    "NUM_BUCKETS = 8\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "total_steps_processed = 0\n",
    "n_modules_to_replace = len(list(filter(lambda x: isinstance(x, LlamaAttention), hash_model.modules())))\n",
    "while n_modules_to_replace > 0:\n",
    "    print(f\"{n_modules_to_replace} LlamaAttention modules remaining\")\n",
    "    # Patch llama attention modules into hash attention modules, one at a time\n",
    "    monkeypatch(hash_model, NUM_BUCKETS, one_at_a_time = True)\n",
    "    n_modules_to_replace -= 1\n",
    "    hash_model.train()\n",
    "\n",
    "    # Freeze all but the hash attention modules\n",
    "    for p in hash_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    for name, module in hash_model.named_modules():\n",
    "        if isinstance(module, HashAttention):\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.AdamW(hash_model.parameters(), lr=1e-5)\n",
    "    hash_model.train()\n",
    "\n",
    "    # Training loop parameters\n",
    "    num_steps = 1e3\n",
    "    max_length = 512\n",
    "    ema_alpha = 0.05  # EMA smoothing factor\n",
    "\n",
    "    step = 0\n",
    "    ema_loss = None\n",
    "    ema_perplexity = None\n",
    "\n",
    "    # Use streaming dataset with tqdm\n",
    "    with tqdm(total=num_steps, desc=\"Training\") as pbar:\n",
    "        for example in fineweb_dataset:\n",
    "            if step >= num_steps:\n",
    "                total_steps_processed += step\n",
    "                fineweb_dataset = fineweb_dataset.skip(total_steps_processed) # continue from where we left off\n",
    "                break\n",
    "                \n",
    "            text = example['text']\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, \n",
    "                            truncation=True, padding=True)\n",
    "            inputs = {k: v.to(hash_model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = hash_model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss).item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update EMA metrics\n",
    "            current_loss = loss.item()\n",
    "            if ema_loss is None:\n",
    "                ema_loss = current_loss\n",
    "                ema_perplexity = perplexity\n",
    "            else:\n",
    "                ema_loss = ema_alpha * current_loss + (1 - ema_alpha) * ema_loss\n",
    "                ema_perplexity = ema_alpha * perplexity + (1 - ema_alpha) * ema_perplexity\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            # Update pbar\n",
    "            pbar.set_description(f\"Training - Loss: {ema_loss:.4f}, PPL: {ema_perplexity:.2f}\")\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8132529f3aa54df38a8e9eda529b8233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing KL divergences between flash and hash attention models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:46<00:00, 21.65it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load a slice of the FineWeb dataset\n",
    "fineweb_dataset = datasets.load_dataset(\"HuggingFaceFW/fineweb\", \"sample-10BT\", split=\"train\", streaming=True)\n",
    "fineweb_sample = fineweb_dataset.take(1000) \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Function to compute KL divergence between original and hash model\n",
    "def compute_fineweb_metrics(original_model, hash_model, tokenizer, texts, max_length=512):\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    kl_divergences = []\n",
    "    original_perplexities = []\n",
    "    hash_perplexities = []\n",
    "    \n",
    "    for text in tqdm(texts):\n",
    "        # Tokenize the text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=True)\n",
    "        inputs = {k: v.to(original_model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get logits from both models\n",
    "            original_outputs = original_model(**inputs)\n",
    "            hash_outputs = hash_model(**inputs)\n",
    "            \n",
    "            original_logits = original_outputs.logits\n",
    "            hash_logits = hash_outputs.logits\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            original_probs = F.softmax(original_logits, dim=-1)\n",
    "            hash_probs = F.softmax(hash_logits, dim=-1)\n",
    "            \n",
    "            # Compute KL divergence: KL(original || hash)\n",
    "            kl_div = F.kl_div(hash_probs.log(), original_probs, reduction='batchmean')\n",
    "            kl_divergences.append(kl_div.item())\n",
    "            \n",
    "            # Compute perplexities\n",
    "            # Shift logits and labels for next token prediction\n",
    "            shift_logits_original = original_logits[..., :-1, :].contiguous()\n",
    "            shift_logits_hash = hash_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = inputs['input_ids'][..., 1:].contiguous()\n",
    "            \n",
    "            # Compute cross entropy loss\n",
    "            original_loss = F.cross_entropy(shift_logits_original.view(-1, shift_logits_original.size(-1)), \n",
    "                                          shift_labels.view(-1), reduction='mean')\n",
    "            hash_loss = F.cross_entropy(shift_logits_hash.view(-1, shift_logits_hash.size(-1)), \n",
    "                                      shift_labels.view(-1), reduction='mean')\n",
    "            \n",
    "            # Convert loss to perplexity\n",
    "            original_perplexity = torch.exp(original_loss).item()\n",
    "            hash_perplexity = torch.exp(hash_loss).item()\n",
    "            \n",
    "            original_perplexities.append(original_perplexity)\n",
    "            hash_perplexities.append(hash_perplexity)\n",
    "    \n",
    "    return {\n",
    "        'kl_divergences': kl_divergences,\n",
    "        'original_perplexities': original_perplexities,\n",
    "        'hash_perplexities': hash_perplexities\n",
    "    }\n",
    "# Extract texts from the dataset\n",
    "texts = [sample['text'] for sample in fineweb_sample]\n",
    "\n",
    "print(\"Computing KL divergences between flash and hash attention models...\")\n",
    "kl_divs = compute_fineweb_metrics(model, hash_model, tokenizer, texts)\n",
    "\n",
    "# Create the bar plot\n",
    "ax = sns.barplot(x=[\"KL Divergence\", \"Original Perplexity\", \"Hash Perplexity\"], \n",
    "                 y=[np.mean(kl_divs['kl_divergences']), \n",
    "                    np.mean(kl_divs['original_perplexities']), \n",
    "                    np.mean(kl_divs['hash_perplexities'])])\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, v in enumerate([np.mean(kl_divs['kl_divergences']), \n",
    "                       np.mean(kl_divs['original_perplexities']), \n",
    "                       np.mean(kl_divs['hash_perplexities'])]):\n",
    "    ax.text(i, v + v*0.01, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.title('Comparison of KL Divergence and Perplexities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('checkpoints/hash_model_learnable/tokenizer_config.json',\n",
       " 'checkpoints/hash_model_learnable/special_tokens_map.json',\n",
       " 'checkpoints/hash_model_learnable/chat_template.jinja',\n",
       " 'checkpoints/hash_model_learnable/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_model.save_pretrained(\"checkpoints/hash_model_learnable\")\n",
    "tokenizer.save_pretrained(\"checkpoints/hash_model_learnable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwz-ml\u001b[0m (\u001b[33mcoactivelearning\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.20.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/will/important/Triton/SFCA/experiments/training/wandb/run-20250606_215846-e2qcy8ai\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdeft-feather-18\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/coactivelearning/lm-eval-harness-integration\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/coactivelearning/lm-eval-harness-integration/runs/e2qcy8ai\u001b[0m\n",
      "2025-06-06:21:58:51 INFO     [__main__:440] Selected Tasks: ['arc_easy', 'boolq', 'hellaswag', 'lambada_openai', 'mmlu_abstract_algebra', 'piqa', 'winogrande']\n",
      "2025-06-06:21:58:51 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-06-06:21:58:51 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': 'checkpoints/hash_model_learnable'}\n",
      "2025-06-06:21:58:52 INFO     [models.huggingface:137] Using device 'cuda:1'\n",
      "2025-06-06:21:58:52 INFO     [models.huggingface:382] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:1'}\n",
      "Some weights of the model checkpoint at checkpoints/hash_model_learnable were not used when initializing LlamaForCausalLM: ['model.layers.0.self_attn.vector_hash.R']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-06-06:21:59:04 WARNING  [api.task:844] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2025-06-06:21:59:04 WARNING  [api.task:856] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2025-06-06:21:59:18 INFO     [api.task:434] Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████| 1267/1267 [00:00<00:00, 179333.28it/s]\n",
      "2025-06-06:21:59:18 INFO     [api.task:434] Building contexts for piqa on rank 0...\n",
      "100%|█████████████████████████████████████| 1838/1838 [00:00<00:00, 1903.60it/s]\n",
      "2025-06-06:21:59:19 INFO     [api.task:434] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 1127.64it/s]\n",
      "2025-06-06:21:59:19 INFO     [api.task:434] Building contexts for lambada_openai on rank 0...\n",
      "100%|██████████████████████████████████████| 5153/5153 [00:05<00:00, 994.55it/s]\n",
      "2025-06-06:21:59:24 INFO     [api.task:434] Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:02<00:00, 4251.62it/s]\n",
      "2025-06-06:21:59:27 INFO     [api.task:434] Building contexts for boolq on rank 0...\n",
      "100%|█████████████████████████████████████| 3270/3270 [00:00<00:00, 3453.71it/s]\n",
      "2025-06-06:21:59:28 INFO     [api.task:434] Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████| 2376/2376 [00:01<00:00, 1960.56it/s]\n",
      "2025-06-06:21:59:30 INFO     [evaluator:555] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|                 | 0/67972 [00:00<?, ?it/s]Passed argument batch_size = auto:8.0. Detecting largest batch size\n",
      "Determined largest batch size: 16\n",
      "Running loglikelihood requests:  15%|▋    | 9998/67972 [01:02<05:22, 179.80it/s]Passed argument batch_size = auto:8.0. Detecting largest batch size\n",
      "Determined largest batch size: 64\n",
      "Running loglikelihood requests: 100%|████| 67972/67972 [03:47<00:00, 299.33it/s]\n",
      "bootstrapping for stddev: perplexity\n",
      "100%|█████████████████████████████████████████| 100/100 [00:02<00:00, 48.86it/s]\n",
      "2025-06-06:22:03:55 INFO     [loggers.evaluation_tracker:209] Saving results aggregated\n",
      "2025-06-06:22:03:55 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: arc_easy\n",
      "2025-06-06:22:04:01 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: boolq\n",
      "2025-06-06:22:04:11 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: hellaswag\n",
      "2025-06-06:22:04:42 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: lambada_openai\n",
      "2025-06-06:22:04:54 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: mmlu_abstract_algebra\n",
      "2025-06-06:22:04:54 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: piqa\n",
      "2025-06-06:22:04:59 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: winogrande\n",
      "hf (pretrained=checkpoints/hash_model_learnable), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:8 (16,64,64,64,64,64,64,64,64)\n",
      "|     Tasks      |Version|Filter|n-shot|  Metric  |   |  Value  |   | Stderr |\n",
      "|----------------|------:|------|-----:|----------|---|--------:|---|-------:|\n",
      "|arc_easy        |      1|none  |     0|acc       |↑  |   0.2946|±  |  0.0094|\n",
      "|                |       |none  |     0|acc_norm  |↑  |   0.2900|±  |  0.0093|\n",
      "|boolq           |      2|none  |     0|acc       |↑  |   0.4654|±  |  0.0087|\n",
      "|hellaswag       |      1|none  |     0|acc       |↑  |   0.2983|±  |  0.0046|\n",
      "|                |       |none  |     0|acc_norm  |↑  |   0.3282|±  |  0.0047|\n",
      "|lambada_openai  |      1|none  |     0|acc       |↑  |   0.1958|±  |  0.0055|\n",
      "|                |       |none  |     0|perplexity|↓  |1859.8136|±  |143.6249|\n",
      "|abstract_algebra|      1|none  |     0|acc       |↑  |   0.2300|±  |  0.0423|\n",
      "|piqa            |      1|none  |     0|acc       |↑  |   0.5528|±  |  0.0116|\n",
      "|                |       |none  |     0|acc_norm  |↑  |   0.5207|±  |  0.0117|\n",
      "|winogrande      |      1|none  |     0|acc       |↑  |   0.5328|±  |  0.0140|\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     arc_easy/acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                arc_easy/acc_norm ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         arc_easy/acc_norm_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              arc_easy/acc_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        boolq/acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 boolq/acc_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    hellaswag/acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               hellaswag/acc_norm ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        hellaswag/acc_norm_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             hellaswag/acc_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               lambada_openai/acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        lambada_openai/acc_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        lambada_openai/perplexity ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: lambada_openai/perplexity_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_abstract_algebra/acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: mmlu_abstract_algebra/acc_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         piqa/acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    piqa/acc_norm ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             piqa/acc_norm_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  piqa/acc_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   winogrande/acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            winogrande/acc_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     arc_easy/acc 0.29461\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                arc_easy/acc_norm 0.28998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         arc_easy/acc_norm_stderr 0.00931\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              arc_easy/acc_stderr 0.00935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   arc_easy/alias arc_easy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        boolq/acc 0.46544\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 boolq/acc_stderr 0.00872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      boolq/alias boolq\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    hellaswag/acc 0.29835\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               hellaswag/acc_norm 0.32822\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        hellaswag/acc_norm_stderr 0.00469\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             hellaswag/acc_stderr 0.00457\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  hellaswag/alias hellaswag\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               lambada_openai/acc 0.19581\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        lambada_openai/acc_stderr 0.00553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             lambada_openai/alias lambada_openai\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        lambada_openai/perplexity 1859.81363\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: lambada_openai/perplexity_stderr 143.62486\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_abstract_algebra/acc 0.23\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: mmlu_abstract_algebra/acc_stderr 0.0423\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      mmlu_abstract_algebra/alias abstract_algebra\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         piqa/acc 0.55277\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    piqa/acc_norm 0.52067\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             piqa/acc_norm_stderr 0.01166\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  piqa/acc_stderr 0.0116\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       piqa/alias piqa\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   winogrande/acc 0.53275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            winogrande/acc_stderr 0.01402\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 winogrande/alias winogrande\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdeft-feather-18\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/coactivelearning/lm-eval-harness-integration/runs/e2qcy8ai\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/coactivelearning/lm-eval-harness-integration\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 8 media file(s), 32 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250606_215846-e2qcy8ai/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained='checkpoints/hash_model_learnable' \\\n",
    "    --tasks lambada_openai,hellaswag,piqa,boolq,winogrande,arc_easy,mmlu_abstract_algebra \\\n",
    "    --device cuda:1 \\\n",
    "    --batch_size auto:8 \\\n",
    "    --wandb_args project=lm-eval-harness-integration \\\n",
    "    --output_path eval_results \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
